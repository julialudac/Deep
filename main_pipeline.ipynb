{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FramesAtGivenScaledImage import FramesAtGivenScaledImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#personal imports\n",
    "from configuration import *\n",
    "from CustomDatasetFromImages import CustomDatasetFromImages\n",
    "from cut_image import Chunking\n",
    "from detections import *\n",
    "from DetectionCandidate import *\n",
    "from FramesAtGivenScaledImage import FramesAtGivenScaledImage\n",
    "from ImageWithDetections import ImageWithDetections\n",
    "import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(train_loader, net, optimizer, criterion):\n",
    "    print('Start training')\n",
    "    for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_neural_network_original_dataset(test_loader, net):\n",
    "    class_correct = list(0. for i in range(2))\n",
    "    class_total = list(0. for i in range(2))\n",
    "    classes = (0, 1,)\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            labels = torch.LongTensor(([1 if label > 1 else label for label in labels ]))\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    for i in range(2):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            labels = torch.LongTensor(([1 if label > 1 else label for label in labels ]))\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "            100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_neural_network_own_dataset(path_to_image, net):\n",
    "    im = Image.open(path_to_image)\n",
    "    # Convert into grayscale\n",
    "    #im = im.convert(\"L\")\n",
    "\n",
    "    #cut image to different scales\n",
    "    Chunking.init(im)\n",
    "    (scaled_images, scaled_positions) = Chunking.get_imgchunks_atdiffscales(strides=(5, 5), nbshrinkages=3, divfactor=2)\n",
    "\n",
    "    #compute score for each frame\n",
    "    frames = []\n",
    "    scaled_factor = 1\n",
    "    for index in range(len(scaled_images)):\n",
    "        print(scaled_factor)\n",
    "        dataset = CustomDatasetFromImages(scaled_images[index])\n",
    "        ##test on neural network\n",
    "        #get the dataloader. Minibatches of 4\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=4,\n",
    "                                                 shuffle=True, num_workers=2)\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for data in dataloader:\n",
    "                images, _ = data\n",
    "                outputs = net(images) #[(score of class_0, score of class_1), ...]\n",
    "                outputs = outputs.numpy()\n",
    "                outputs = outputs.tolist()\n",
    "                scores.extend(outputs)\n",
    "        frames.append(FramesAtGivenScaledImage(scaled_factor, dataset, scaled_positions[index], scores))\n",
    "        scaled_factor = division_factor*(2**index)\n",
    "\n",
    "    #subdetections\n",
    "    subdetections = capture_good_positions(frames)\n",
    "    print(\"subdetections:\")\n",
    "    for subd in subdetections:\n",
    "        print(subd)\n",
    "\n",
    "    \"\"\"3/ Clustering of DetectionCandidates into Detections and filtering\"\"\"\n",
    "    detections = getDetections(subdetections, min_samples=2)  # min_samples --> number of min detection to confirm the detection\n",
    "    print(detections)  # In the example, one subdetection is alon in a detection => this detection is discarded.\n",
    "\n",
    "    \"\"\"4/ Get the best candidates\"\"\"\n",
    "    winners = get_best_clusters_candidates(subdetections, detections)\n",
    "    for w in winners:\n",
    "        print(w)\n",
    "\n",
    "    \"\"\"5/ Save an image with all subdetections, and then only with the kept subdetections\"\"\"\n",
    "\n",
    "    # With all subdetections\n",
    "    imdet = ImageWithDetections(im, subdetections)\n",
    "    original_name_image = path_to_image.split('.')\n",
    "    imdet.save(original_name_image[0]+\"_all_detections.JPG\")\n",
    "\n",
    "    # With only winner subdetections\n",
    "    imdet.subdetections = winners\n",
    "    imdet.save(original_name_image[0]+\"_winners_detections.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data set\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5], [0.5])])\n",
    "train_imagenet = torchvision.datasets.ImageFolder('start_deep/start_deep/start_deep/train_images/',\n",
    "                                                  transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_imagenet, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "#define net and define optimizer\n",
    "net = Net.Net()\n",
    "weights = torch.Tensor([1, 0.5])\n",
    "criterion = nn.CrossEntropyLoss(weights)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "[1,  2000] loss: 0.388\n",
      "[1,  4000] loss: 0.165\n",
      "[1,  6000] loss: 0.127\n",
      "[1,  8000] loss: 0.114\n",
      "[1, 10000] loss: 0.091\n",
      "[1, 12000] loss: 0.081\n",
      "[1, 14000] loss: 0.067\n",
      "[1, 16000] loss: 0.058\n",
      "[1, 18000] loss: 0.048\n",
      "[1, 20000] loss: 0.052\n",
      "[1, 22000] loss: 0.041\n",
      "[2,  2000] loss: 0.037\n",
      "[2,  4000] loss: 0.029\n",
      "[2,  6000] loss: 0.032\n",
      "[2,  8000] loss: 0.029\n",
      "[2, 10000] loss: 0.031\n",
      "[2, 12000] loss: 0.028\n",
      "[2, 14000] loss: 0.019\n",
      "[2, 16000] loss: 0.019\n",
      "[2, 18000] loss: 0.022\n",
      "[2, 20000] loss: 0.016\n",
      "[2, 22000] loss: 0.022\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "net = train_neural_network(train_loader, net, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of     0 : 98 %\n",
      "Accuracy of     1 : 31 %\n",
      "Accuracy of the network on the 10000 test images: 91 %\n"
     ]
    }
   ],
   "source": [
    "#test with original dataset\n",
    "test_imagenet = torchvision.datasets.ImageFolder('start_deep/start_deep/start_deep/test_images/',\n",
    "                                                 transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_imagenet, batch_size=4, shuffle=True, num_workers=2)\n",
    "test_neural_network_original_dataset(test_loader, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ex : own_dataset/ex3.JPG). The path to the image to test is :own_dataset/ex5.JPG\n",
      "1\n",
      "1.2\n",
      "2.4\n",
      "4.8\n",
      "subdetections:\n",
      "score:7.071359157562256; position:(0, 35); dims:(36, 36)\n",
      "score:1.894158661365509; position:(0, 115); dims:(36, 36)\n",
      "score:1.1626574993133545; position:(5, 30); dims:(36, 36)\n",
      "score:19.297537803649902; position:(10, 40); dims:(36, 36)\n",
      "score:1.6784915924072266; position:(10, 85); dims:(36, 36)\n",
      "score:0.9707764983177185; position:(10, 265); dims:(36, 36)\n",
      "score:1.1851617693901062; position:(10, 305); dims:(36, 36)\n",
      "score:2.1951162815093994; position:(15, 35); dims:(36, 36)\n",
      "score:5.585206031799316; position:(15, 324); dims:(36, 36)\n",
      "score:0.33121080696582794; position:(20, 75); dims:(36, 36)\n",
      "score:0.997149646282196; position:(20, 90); dims:(36, 36)\n",
      "score:1.151901662349701; position:(20, 320); dims:(36, 36)\n",
      "score:0.30714984238147736; position:(25, 75); dims:(36, 36)\n",
      "score:17.428054809570312; position:(25, 115); dims:(36, 36)\n",
      "score:10.211194038391113; position:(30, 5); dims:(36, 36)\n",
      "score:0.3871677666902542; position:(30, 180); dims:(36, 36)\n",
      "score:2.58367657661438; position:(30, 190); dims:(36, 36)\n",
      "score:2.1770600080490112; position:(40, 0); dims:(36, 36)\n",
      "score:3.816716194152832; position:(40, 35); dims:(36, 36)\n",
      "score:7.629159688949585; position:(40, 280); dims:(36, 36)\n",
      "score:4.500585556030273; position:(45, 160); dims:(36, 36)\n",
      "score:10.453125; position:(45, 255); dims:(36, 36)\n",
      "score:10.398153305053711; position:(50, 0); dims:(36, 36)\n",
      "score:0.7482815682888031; position:(50, 130); dims:(36, 36)\n",
      "score:13.974030017852783; position:(55, 285); dims:(36, 36)\n",
      "score:1.3075562715530396; position:(60, 0); dims:(36, 36)\n",
      "score:1.8258341550827026; position:(60, 25); dims:(36, 36)\n",
      "score:1.7172472476959229; position:(60, 230); dims:(36, 36)\n",
      "score:10.613068103790283; position:(60, 270); dims:(36, 36)\n",
      "score:1.0490404963493347; position:(60, 290); dims:(36, 36)\n",
      "score:4.350719332695007; position:(65, 0); dims:(36, 36)\n",
      "score:6.464063882827759; position:(65, 145); dims:(36, 36)\n",
      "score:0.724280834197998; position:(65, 240); dims:(36, 36)\n",
      "score:4.195836186408997; position:(70, 190); dims:(36, 36)\n",
      "score:0.7963950932025909; position:(75, 40); dims:(36, 36)\n",
      "score:0.03004857897758484; position:(75, 150); dims:(36, 36)\n",
      "score:1.3285198211669922; position:(75, 315); dims:(36, 36)\n",
      "score:1.271602988243103; position:(80, 0); dims:(36, 36)\n",
      "score:0.11744798719882965; position:(80, 65); dims:(36, 36)\n",
      "score:7.416618347167969; position:(85, 20); dims:(36, 36)\n",
      "score:8.526163101196289; position:(85, 225); dims:(36, 36)\n",
      "score:1.9768521189689636; position:(85, 320); dims:(36, 36)\n",
      "score:0.18445362150669098; position:(90, 175); dims:(36, 36)\n",
      "score:7.492527484893799; position:(105, 25); dims:(36, 36)\n",
      "score:1.6319766640663147; position:(105, 35); dims:(36, 36)\n",
      "score:0.9810259342193604; position:(105, 165); dims:(36, 36)\n",
      "score:0.41485193371772766; position:(110, 95); dims:(36, 36)\n",
      "score:16.536824226379395; position:(110, 115); dims:(36, 36)\n",
      "score:14.6350736618042; position:(110, 120); dims:(36, 36)\n",
      "score:0.7516486942768097; position:(115, 140); dims:(36, 36)\n",
      "score:0.5595765709877014; position:(115, 180); dims:(36, 36)\n",
      "score:1.46147221326828; position:(115, 195); dims:(36, 36)\n",
      "score:9.969738006591797; position:(120, 110); dims:(36, 36)\n",
      "score:4.158501982688904; position:(120, 280); dims:(36, 36)\n",
      "score:7.743963241577148; position:(125, 295); dims:(36, 36)\n",
      "score:3.723525643348694; position:(130, 305); dims:(36, 36)\n",
      "score:5.958014965057373; position:(135, 125); dims:(36, 36)\n",
      "score:1.4217860698699951; position:(135, 205); dims:(36, 36)\n",
      "score:6.456486463546753; position:(140, 105); dims:(36, 36)\n",
      "score:11.135546207427979; position:(140, 165); dims:(36, 36)\n",
      "score:1.4637057185173035; position:(145, 60); dims:(36, 36)\n",
      "score:1.9499055743217468; position:(145, 255); dims:(36, 36)\n",
      "score:4.399279594421387; position:(155, 250); dims:(36, 36)\n",
      "score:1.0021742582321167; position:(160, 30); dims:(36, 36)\n",
      "score:11.427371978759766; position:(165, 60); dims:(36, 36)\n",
      "score:7.513041973114014; position:(165, 135); dims:(36, 36)\n",
      "score:0.34747709333896637; position:(165, 215); dims:(36, 36)\n",
      "score:0.8583201617002487; position:(170, 5); dims:(36, 36)\n",
      "score:3.3334786891937256; position:(170, 60); dims:(36, 36)\n",
      "score:2.1883411407470703; position:(170, 75); dims:(36, 36)\n",
      "score:2.4163365364074707; position:(170, 80); dims:(36, 36)\n",
      "score:1.9973936676979065; position:(170, 180); dims:(36, 36)\n",
      "score:1.1926992535591125; position:(170, 324); dims:(36, 36)\n",
      "score:3.2178820371627808; position:(175, 35); dims:(36, 36)\n",
      "score:13.325805187225342; position:(175, 175); dims:(36, 36)\n",
      "score:1.7512075901031494; position:(175, 205); dims:(36, 36)\n",
      "score:19.40349578857422; position:(175, 240); dims:(36, 36)\n",
      "score:13.512784004211426; position:(175, 280); dims:(36, 36)\n",
      "score:4.26279354095459; position:(175, 324); dims:(36, 36)\n",
      "score:5.5392005443573; position:(180, 175); dims:(36, 36)\n",
      "score:2.6979730129241943; position:(180, 250); dims:(36, 36)\n",
      "score:1.5312979817390442; position:(185, 10); dims:(36, 36)\n",
      "score:3.472084403038025; position:(195, 0); dims:(36, 36)\n",
      "score:1.346733570098877; position:(195, 55); dims:(36, 36)\n",
      "score:2.9614672660827637; position:(200, 235); dims:(36, 36)\n",
      "score:1.4150050282478333; position:(200, 245); dims:(36, 36)\n",
      "score:2.1275808215141296; position:(210, 55); dims:(36, 36)\n",
      "score:14.923095703125; position:(210, 220); dims:(36, 36)\n",
      "score:1.4871620535850525; position:(225, 0); dims:(36, 36)\n",
      "score:3.4141582250595093; position:(225, 150); dims:(36, 36)\n",
      "score:1.4871947169303894; position:(225, 225); dims:(36, 36)\n",
      "score:0.4470909982919693; position:(225, 315); dims:(36, 36)\n",
      "score:2.2963963747024536; position:(230, 95); dims:(36, 36)\n",
      "score:0.38783296197652817; position:(230, 175); dims:(36, 36)\n",
      "score:5.642723560333252; position:(235, 55); dims:(36, 36)\n",
      "score:1.9051189422607422; position:(235, 75); dims:(36, 36)\n",
      "score:9.223235607147217; position:(235, 80); dims:(36, 36)\n",
      "score:7.294294834136963; position:(235, 260); dims:(36, 36)\n",
      "score:1.410018801689148; position:(240, 95); dims:(36, 36)\n",
      "score:2.7764620780944824; position:(240, 185); dims:(36, 36)\n",
      "score:1.16897451877594; position:(240, 220); dims:(36, 36)\n",
      "score:3.1446231603622437; position:(240, 250); dims:(36, 36)\n",
      "score:0.9097471237182617; position:(240, 320); dims:(36, 36)\n",
      "score:6.979886054992676; position:(245, 135); dims:(36, 36)\n",
      "score:6.192652940750122; position:(245, 155); dims:(36, 36)\n",
      "score:12.793704986572266; position:(250, 25); dims:(36, 36)\n",
      "score:12.926342964172363; position:(250, 155); dims:(36, 36)\n",
      "score:19.5540189743042; position:(250, 180); dims:(36, 36)\n",
      "score:1.6827239990234375; position:(250, 295); dims:(36, 36)\n",
      "score:3.2596991062164307; position:(250, 320); dims:(36, 36)\n",
      "score:1.3153531551361084; position:(255, 255); dims:(36, 36)\n",
      "score:10.64579153060913; position:(255, 315); dims:(36, 36)\n",
      "score:9.52890682220459; position:(265, 5); dims:(36, 36)\n",
      "score:6.618668079376221; position:(265, 165); dims:(36, 36)\n",
      "score:0.4139604717493057; position:(270, 20); dims:(36, 36)\n",
      "score:0.10513298213481903; position:(270, 140); dims:(36, 36)\n",
      "score:14.439701080322266; position:(270, 220); dims:(36, 36)\n",
      "score:1.3594939708709717; position:(270, 250); dims:(36, 36)\n",
      "score:1.710556447505951; position:(275, 5); dims:(36, 36)\n",
      "score:1.0001196265220642; position:(275, 100); dims:(36, 36)\n",
      "score:0.5101504474878311; position:(275, 255); dims:(36, 36)\n",
      "score:10.334533214569092; position:(280, 295); dims:(36, 36)\n",
      "score:6.856271982192993; position:(290, 105); dims:(36, 36)\n",
      "score:1.777993619441986; position:(290, 125); dims:(36, 36)\n",
      "score:2.0566234588623047; position:(290, 150); dims:(36, 36)\n",
      "score:4.2403963804244995; position:(290, 160); dims:(36, 36)\n",
      "score:5.9562296867370605; position:(290, 255); dims:(36, 36)\n",
      "score:2.203980565071106; position:(290, 300); dims:(36, 36)\n",
      "score:3.5176762342453003; position:(290, 305); dims:(36, 36)\n",
      "score:2.7662744522094727; position:(300, 90); dims:(36, 36)\n",
      "score:0.8717109560966492; position:(300, 175); dims:(36, 36)\n",
      "score:8.34969711303711; position:(300, 210); dims:(36, 36)\n",
      "score:0.3270080089569092; position:(305, 5); dims:(36, 36)\n",
      "score:4.925153732299805; position:(305, 20); dims:(36, 36)\n",
      "score:6.9455647468566895; position:(305, 100); dims:(36, 36)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:9.81891918182373; position:(305, 155); dims:(36, 36)\n",
      "score:0.9079889357089996; position:(305, 180); dims:(36, 36)\n",
      "score:2.7703839540481567; position:(305, 230); dims:(36, 36)\n",
      "score:4.492739915847778; position:(305, 255); dims:(36, 36)\n",
      "score:11.299095630645752; position:(305, 315); dims:(36, 36)\n",
      "score:9.352459907531738; position:(310, 310); dims:(36, 36)\n",
      "score:8.27160096168518; position:(315, 55); dims:(36, 36)\n",
      "score:13.870831489562988; position:(315, 280); dims:(36, 36)\n",
      "score:2.060836434364319; position:(324, 15); dims:(36, 36)\n",
      "score:4.639812231063843; position:(324, 85); dims:(36, 36)\n",
      "score:0.18997175991535187; position:(324, 110); dims:(36, 36)\n",
      "score:2.286090672016144; position:(324, 125); dims:(36, 36)\n",
      "score:4.079974293708801; position:(0, 96); dims:(43, 43)\n",
      "score:2.6338096857070923; position:(0, 102); dims:(43, 43)\n",
      "score:1.9276899099349976; position:(0, 144); dims:(43, 43)\n",
      "score:3.362184762954712; position:(6, 36); dims:(43, 43)\n",
      "score:2.6540225744247437; position:(6, 48); dims:(43, 43)\n",
      "score:3.922229290008545; position:(12, 0); dims:(43, 43)\n",
      "score:7.2018938064575195; position:(12, 6); dims:(43, 43)\n",
      "score:1.908521056175232; position:(12, 18); dims:(43, 43)\n",
      "score:1.7111448645591736; position:(12, 30); dims:(43, 43)\n",
      "score:2.0543397665023804; position:(12, 96); dims:(43, 43)\n",
      "score:5.654949188232422; position:(12, 156); dims:(43, 43)\n",
      "score:4.5637147426605225; position:(18, 120); dims:(43, 43)\n",
      "score:2.0647144317626953; position:(24, 6); dims:(43, 43)\n",
      "score:2.7535241842269897; position:(24, 24); dims:(43, 43)\n",
      "score:0.9113254547119141; position:(24, 102); dims:(43, 43)\n",
      "score:2.6303244829177856; position:(30, 60); dims:(43, 43)\n",
      "score:0.011559620499610901; position:(30, 66); dims:(43, 43)\n",
      "score:0.6027635335922241; position:(30, 114); dims:(43, 43)\n",
      "score:0.5746825635433197; position:(36, 42); dims:(43, 43)\n",
      "score:0.3576626777648926; position:(36, 150); dims:(43, 43)\n",
      "score:3.2556216716766357; position:(42, 6); dims:(43, 43)\n",
      "score:0.02982248365879059; position:(42, 102); dims:(43, 43)\n",
      "score:1.0920168161392212; position:(42, 126); dims:(43, 43)\n",
      "score:3.247471570968628; position:(48, 108); dims:(43, 43)\n",
      "score:0.7065452039241791; position:(60, 48); dims:(43, 43)\n",
      "score:0.8548136651515961; position:(60, 126); dims:(43, 43)\n",
      "score:1.3448660373687744; position:(66, 36); dims:(43, 43)\n",
      "score:2.6715447902679443; position:(72, 24); dims:(43, 43)\n",
      "score:3.7585442066192627; position:(72, 114); dims:(43, 43)\n",
      "score:2.137511968612671; position:(72, 120); dims:(43, 43)\n",
      "score:1.1802214980125427; position:(78, 18); dims:(43, 43)\n",
      "score:4.823474645614624; position:(78, 132); dims:(43, 43)\n",
      "score:2.9912534952163696; position:(90, 18); dims:(43, 43)\n",
      "score:2.4575599431991577; position:(90, 60); dims:(43, 43)\n",
      "score:1.5112212896347046; position:(102, 162); dims:(43, 43)\n",
      "score:0.07543197274208069; position:(108, 18); dims:(43, 43)\n",
      "score:0.574150338768959; position:(108, 36); dims:(43, 43)\n",
      "score:0.3493790179491043; position:(108, 132); dims:(43, 43)\n",
      "score:1.926510214805603; position:(120, 78); dims:(43, 43)\n",
      "score:1.6360441446304321; position:(120, 96); dims:(43, 43)\n",
      "score:2.9099167585372925; position:(120, 108); dims:(43, 43)\n",
      "score:2.8920403718948364; position:(120, 144); dims:(43, 43)\n",
      "score:5.307564735412598; position:(126, 66); dims:(43, 43)\n",
      "score:0.4148043692111969; position:(132, 120); dims:(43, 43)\n",
      "score:0.7445479780435562; position:(138, 108); dims:(43, 43)\n",
      "score:0.5680951774120331; position:(138, 138); dims:(43, 43)\n",
      "score:0.5099688917398453; position:(150, 6); dims:(43, 43)\n",
      "score:2.1468425989151; position:(150, 66); dims:(43, 43)\n",
      "score:2.3351064920425415; position:(150, 132); dims:(43, 43)\n",
      "score:8.485334873199463; position:(150, 162); dims:(43, 43)\n",
      "score:1.1354444026947021; position:(150, 168); dims:(43, 43)\n",
      "score:2.110963463783264; position:(156, 132); dims:(43, 43)\n",
      "score:0.9597352743148804; position:(156, 172); dims:(43, 43)\n",
      "score:0.5088021010160446; position:(162, 108); dims:(43, 43)\n",
      "score:0.49684347212314606; position:(168, 0); dims:(43, 43)\n",
      "score:2.761385202407837; position:(168, 18); dims:(43, 43)\n",
      "score:1.3559749126434326; position:(168, 120); dims:(43, 43)\n",
      "score:0.40757161378860474; position:(172, 30); dims:(43, 43)\n",
      "score:3.8564531803131104; position:(172, 150); dims:(43, 43)\n",
      "score:5.2729811668396; position:(12, 120); dims:(86, 86)\n",
      "score:3.0883891582489014; position:(24, 48); dims:(86, 86)\n",
      "score:1.2336105108261108; position:(24, 129); dims:(86, 86)\n",
      "score:3.2165310382843018; position:(120, 60); dims:(86, 86)\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]]\n",
      "score:19.5540189743042; position:(250, 180); dims:(36, 36)\n"
     ]
    }
   ],
   "source": [
    "path_to_image = input(\"(ex : own_dataset/ex3.JPG). The path to the image to test is :\")\n",
    "test_neural_network_own_dataset(path_to_image, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
