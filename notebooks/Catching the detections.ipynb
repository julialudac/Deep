{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and classes for the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For before and now: a class to store, for an image at a given rescale, the slifing wondows data: dataset, positions and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FramesAtGivenScaledImage():\n",
    "    \"\"\"Represents the images chunks for the scaled image at one size. \n",
    "    Instances are made before the NN feeding to store the datasets and the positions of the frames/image chunks, \n",
    "    and also used after the NN feeding to store the scores\"\"\"\n",
    "    \n",
    "    def __init__(self, scaling_factor, dataset, positions):\n",
    "        self.scaling_factor = scaling_factor  # The factor used to scale/reduce the image\n",
    "        self.dataset = dataset  # Dataset of image frames => this is a CustomDatasetFromImages instance\n",
    "        self.positions = positions  # Position tuples (x,y) associated to the frames. Positions are from top left.\n",
    "        self.scores = []  # Scores associated to the frames. These are scalars from -2 to 2. Filled with NN output, method set_scores\n",
    "        \n",
    "    def set_scores(net):\n",
    "        \"\"\"net: Neural network used to compute the scores, given the dataset TODO\"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def get_center(frame_position):\n",
    "    \"\"\"Returns the center associated to frame_position in the scaled image \n",
    "    (so the size of a frame is not yet resized:36x36)\n",
    "    \"\"\"\n",
    "    return (frame_position[0]+18, frame_position[1]+18)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes and methods for getting subdetections from frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionCandidate:\n",
    "    \"\"\"It represents a square that represents a detection in the image rescaled to\n",
    "    normal size. Let we call it a subdetection\"\"\"\n",
    "    def __init__(self, score, position, dims):\n",
    "        \"\"\"dimensions of each detection (in the resized image). For example, if the sliding window\n",
    "        was sliding an image that has been reduced by /1.2, the dimension of each subdetection will\n",
    "        be (1.2*36, 1.2*36)\"\"\"\n",
    "        self.score = score   # Score associated to the image frame the subdetection comes from\n",
    "        self.position = position  # Position associated to the image frame the subdetection comes from, BUT in the rescaled image\n",
    "        self.dims = dims # Dimension tuple (wifth, height) of the image frame the subdetection comes from, BUT in the rescaled image\n",
    "        \n",
    "    def get_center(self):\n",
    "        return (self.position[0]+self.dims[0]/2, self.position[1]+self.dims[1]/2)\n",
    "    \n",
    "    def computer_center_dists(self, other_square): # TODO: useless???\n",
    "        \"\"\"Compute norm 2 between actual square center and another square center\"\"\"\n",
    "        return sqrt((self.position[0] - other_square.position[0])\n",
    "                   *(self.position[0] - other_square.position[0])\n",
    "                   +(self.position[1] - other_square.position[1])\n",
    "                   *(self.position[1] - other_square.position[1]))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"score:\" + str(self.score) + \"; position:\" + str(self.position) + \"; dims:\" + str(self.dims)\n",
    "        \n",
    "\n",
    "def capture_good_positions(framesAtGivenScaledImages): # TODO: seems that the scaling of the positions is false\n",
    "    \"\"\"From the list of framesAtGivenScaledImages, we build a list of DetectionCandidates. These are derived from\n",
    "    the chunks whose associated score is =>0.\n",
    "    Returns a list of DetectionCandidates instance \n",
    "    \"\"\"\n",
    "    detectionCandidates = []\n",
    "    for fagsi in framesAtGivenScaledImages:\n",
    "        print(\"fagsi.scores:\", fagsi.scores)\n",
    "        for i in range(len(fagsi.scores)):\n",
    "            if fagsi.scores[i] >= 0:\n",
    "                newdims = (int(36*fagsi.scaling_factor), int(36*fagsi.scaling_factor))\n",
    "                print(\"newdims:\", newdims)\n",
    "                \"\"\"To get the position in the rescaled image (recall that it is top-left), \n",
    "                we first scale linearly the center, and then from the center, we deduced the new position\n",
    "                \"\"\"\n",
    "                old_center = get_center(fagsi.positions[i]) # center in the scaled image (so before rescaling to original size)\n",
    "                newx = old_center[0]*fagsi.scaling_factor\n",
    "                newx = int(newx - newdims[0]/2)\n",
    "                newy = old_center[1]*fagsi.scaling_factor\n",
    "                newy = int(newy - newdims[1]/2)\n",
    "                \"\"\"detectionCandidates.append(DetectionCandidate(fagsi.scores[i], \n",
    "                                                              (int(fagsi.positions[i][0]*fagsi.scaling_factor),\n",
    "                                                               int(fagsi.positions[i][1]*fagsi.scaling_factor)),\n",
    "                                                               (newdims))\"\"\"\n",
    "                detectionCandidates.append(DetectionCandidate(fagsi.scores[i], (newx, newy), newdims))\n",
    "                                            \n",
    "    return detectionCandidates                                      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for getting detections from subdetections AND filtering them at the same time\n",
    "Should be in a file concerning the clustering/detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def cluster_frames(centers_frames, eps=3, min_samples=1):\n",
    "    \"\"\"\n",
    "    :param eps: maximum distance between two samples\n",
    "    :param min_samples: number of samples in a neighborhood for a point to be a core point\n",
    "    :param centers_frames: List of coordinates of the center of frames (x,y)\n",
    "    :return: A list of lists --> for one list, there is the indexes of the frames\n",
    "    We don't have an explicit class to design a Cluster, which is a list of (subdetection) indices.\n",
    "    \"\"\"\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(centers_frames)\n",
    "    dict_clusters = dict()\n",
    "    for index, value in enumerate(clustering.labels_):\n",
    "        # If -1, it means the subdetection does not have enough neighbor => we ignore it and don't build a cluster from it.\n",
    "        if value != -1:\n",
    "            if value not in dict_clusters:\n",
    "                dict_clusters[value] = [index]\n",
    "            else:\n",
    "                dict_clusters[value].append(index)\n",
    "    return list(dict_clusters.values())\n",
    "\n",
    "\n",
    "def getDetections(subdetections, min_samples=1):\n",
    "    \"\"\"Returns a list of Detections.\n",
    "    Detections are clusters. A Detection is a list of indices of subdetections that represent it.\n",
    "    min_samples: number of minimum subdetections in the detection for the detection to be accepted. Otherwise, \n",
    "    the subdetection is ignored\n",
    "    => With this function, we both do the steps of getting the detections and filtering them (discarding those with\n",
    "    too few subdetections)!\n",
    "    \"\"\"\n",
    "    centers_frames = []\n",
    "    for subd in subdetections:\n",
    "        center = subd.get_center()\n",
    "        centers_frames.append([center[0], center[1]]) # because DBSCAN works with vectors that are lists, not tuples\n",
    "    clusters = cluster_frames(centers_frames, 50, min_samples) # TODO: eps must be proportional to the image dims\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for keeping the best subdetection for each detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def get_best_cluster_candidate(subdetections, cluster):\n",
    "    \n",
    "    subdetections: a list of DetectionCandidates\n",
    "    cluster: a list of subdetection indices\n",
    "    Returns the chosen candidate for given cluster\n",
    "    best_score = -1000\n",
    "    best_candidate_index = -1000\n",
    "    for ind, candidate in enumerate(cluster):\n",
    "        if best_score < candidate.score:\n",
    "            best_score = candidate.score\n",
    "            best_candidate_index = ind\n",
    "    return cluster[best_candidate_index]\"\"\"\n",
    "\n",
    "\n",
    "def get_best_clusters_candidates(subdetections, clusters):\n",
    "    \"\"\"\n",
    "    subdetections: a list of DetectionCandidates\n",
    "    clusters: a list of clusters\n",
    "    Returns the chosen candidate for each cluster \n",
    "    => we have a list of DetectionCandidates. Each one represents the Detection that contains it.\"\"\"\n",
    "    chosen = []\n",
    "    best_score = -1000\n",
    "    best_candidate_index = -1000\n",
    "    for c in clusters:\n",
    "        best_score = -1000\n",
    "        best_candidate_index = -1000\n",
    "        for ind in c: # indices of our list of subdetections\n",
    "            if best_score < subdetections[ind].score:\n",
    "                best_score = subdetections[ind].score\n",
    "                best_candidate_index = ind\n",
    "        chosen.append(subdetections[best_candidate_index])\n",
    "    return chosen\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for saving the detections on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first have a function to have a random color\n",
    "import random\n",
    "\n",
    "def random_color():\n",
    "    return (random.randint(0,255), random.randint(0,255), random.randint(0,255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "class ImageWithDetections:\n",
    "    \n",
    "    def __init__(self, im, subdetections):\n",
    "        self.im = im\n",
    "        self.subdetections = subdetections\n",
    "        \n",
    "    def save(self, filename=\"saved_detections.JPG\"):\n",
    "        \"\"\"Save the image as well as the kept DetectionCanditates.\"\"\"\n",
    "        im = self.im.copy()\n",
    "        draw = ImageDraw.Draw(im)\n",
    "        fntsize = 18\n",
    "        fnt = ImageFont.truetype(\"impact.ttf\",fntsize)\n",
    "        for subd in self.subdetections:\n",
    "            randomc = random_color()\n",
    "            draw.rectangle((subd.position[0],subd.position[1],\n",
    "                            subd.position[0]+subd.dims[0],subd.position[1]+subd.dims[1]), \n",
    "                           outline=randomc) # But we can't specify border width :(\n",
    "            draw.text((subd.position[0],subd.position[1]-fntsize), str(subd.score), fill=randomc, font=fnt)\n",
    "        im.save(filename, \"JPEG\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plaing around around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing a rectangle and writing a text on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "pilImage = Image.open(\"catch_detec_images/IMGP0017.JPG\")\n",
    "draw = ImageDraw.Draw(pilImage) # type: ImageDraw. Its existing affects the image pilImage.\n",
    "# draw.rectangle((100,200, 500,300), outline=\"red\") \n",
    "draw.rectangle((100,200, 500,300), outline=(255,0,0)) # But we can't specify border width :(\n",
    "\n",
    "# text with font size of 100 px.\n",
    "fnt = ImageFont.truetype(\"impact.ttf\",100)\n",
    "draw.text((10,10), \"Hello World\", fill=(255,255,0), font=fnt)\n",
    "\n",
    "\n",
    "pilImage.save(\"catch_detec_images/withdrawing\" + \".JPG\", \"JPEG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing whole pipeline, from the lists of scores and positions at diff scalings (so from FramesAtGivenScaledImage instances) the to the image with detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let we have these FramesAtGivenScaledImage instances, so we have the frames at different scales with positions and refined scores associated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each image size, we won't add all the possible frames, it will be too long. But it's not important to test.\n",
    "# We don't need the dataset to test, this was for the NN step\n",
    "\n",
    "# Will be done before NN feeding\n",
    "fagsi1 = FramesAtGivenScaledImage(1, [], [(0,0), (95,130), (100,130), (500,300)])  \n",
    "fagsi2 = FramesAtGivenScaledImage(1.2, [], [(0,0), (81,105), (333,458), (500,300)])  \n",
    "fagsi3 = FramesAtGivenScaledImage(2.4, [], [(0,1), (38,53), (167,230), (312,33), (400,20)]) \n",
    "\n",
    "# Will be done after NN feeding. TODO; implement set_scores\n",
    "fagsi1.scores = [-1, 0.6, 0.65, -0.4]\n",
    "fagsi2.scores = [-1, 0.7, 0.7, -0.4]\n",
    "fagsi3.scores = [-1.2, 0.53, 0.8, 0.9, -0.1]\n",
    "\n",
    "\n",
    "framesAtGivenScaledImages = []\n",
    "framesAtGivenScaledImages.append(fagsi1)\n",
    "framesAtGivenScaledImages.append(fagsi2)\n",
    "framesAtGivenScaledImages.append(fagsi3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frome these, we get the subdetections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fagsi.scores: [-1, 0.6, 0.65, -0.4]\n",
      "newdims: (36, 36)\n",
      "newdims: (36, 36)\n",
      "fagsi.scores: [-1, 0.7, 0.7, -0.4]\n",
      "newdims: (43, 43)\n",
      "newdims: (43, 43)\n",
      "fagsi.scores: [-1.2, 0.53, 0.8, 0.9, -0.1]\n",
      "newdims: (86, 86)\n",
      "newdims: (86, 86)\n",
      "newdims: (86, 86)\n",
      "subdetections:\n",
      "score:0.6; position:(95, 130); dims:(36, 36)\n",
      "score:0.65; position:(100, 130); dims:(36, 36)\n",
      "score:0.7; position:(97, 126); dims:(43, 43)\n",
      "score:0.7; position:(399, 549); dims:(43, 43)\n",
      "score:0.53; position:(91, 127); dims:(86, 86)\n",
      "score:0.8; position:(401, 552); dims:(86, 86)\n",
      "score:0.9; position:(749, 79); dims:(86, 86)\n"
     ]
    }
   ],
   "source": [
    "subdetections = capture_good_positions(framesAtGivenScaledImages)\n",
    "print(\"subdetections:\")\n",
    "for subd in subdetections:\n",
    "    print(subd)\n",
    "# OK!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of DetectionCandidates into Detections and filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 4], [3, 5]]\n"
     ]
    }
   ],
   "source": [
    "detections = getDetections(subdetections,min_samples=2)  # OK!\n",
    "print(detections) # In the example, one subdetection is alon in a detection => this detection is discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the best candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:0.7; position:(97, 126); dims:(43, 43)\n",
      "score:0.8; position:(401, 552); dims:(86, 86)\n"
     ]
    }
   ],
   "source": [
    "winners = get_best_clusters_candidates(subdetections, detections)\n",
    "for w in winners:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save an image with all subdetections, and then only with the kept subdetections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# With all subdetections\n",
    "pilImage = Image.open(\"catch_detec_images/blank_example.jpg\")\n",
    "imdet = ImageWithDetections(pilImage, subdetections)\n",
    "imdet.save(\"catch_detec_images/all_detections.JPG\")\n",
    "\n",
    "# With only winner subdetections\n",
    "imdet.subdetections = winners\n",
    "imdet.save(\"catch_detec_images/winner_detections.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
