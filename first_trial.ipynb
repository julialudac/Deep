{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas\n",
    "from skimage import io, transform\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 : Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDataset(Dataset):\n",
    "    \"\"\"Images dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, txt_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            txt_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pandas.read_table(txt_file, delim_whitespace=True, names=('name', 'label'))\n",
    "        print(self.data)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.data.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        labels = self.data.iloc[idx, 1:].as_matrix()\n",
    "        labels = labels.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'labels': labels}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             name  label\n",
      "0       0/137021_102_88_72_72.pgm      0\n",
      "1        0/137021_103_5_48_48.pgm      0\n",
      "2       0/137021_115_30_60_60.pgm      0\n",
      "3       0/137021_117_15_36_36.pgm      0\n",
      "4       0/137021_117_38_57_57.pgm      0\n",
      "5       0/137021_121_56_36_36.pgm      0\n",
      "6      0/137021_123_131_36_36.pgm      0\n",
      "7       0/137021_123_93_55_55.pgm      0\n",
      "8         0/137021_12_4_75_75.pgm      0\n",
      "9        0/137021_128_2_38_38.pgm      0\n",
      "10     0/137021_135_119_45_45.pgm      0\n",
      "11      0/137021_138_12_36_36.pgm      0\n",
      "12      0/137021_138_99_36_36.pgm      0\n",
      "13        0/137021_1_42_76_76.pgm      0\n",
      "14       0/137021_14_68_42_42.pgm      0\n",
      "15       0/137021_14_87_86_86.pgm      0\n",
      "16       0/137021_14_94_55_55.pgm      0\n",
      "17       0/137021_16_86_82_82.pgm      0\n",
      "18       0/137021_17_40_73_73.pgm      0\n",
      "19       0/137021_18_65_89_89.pgm      0\n",
      "20        0/137021_19_8_36_36.pgm      0\n",
      "21       0/137021_21_37_36_36.pgm      0\n",
      "22       0/137021_22_43_66_66.pgm      0\n",
      "23       0/137021_25_70_65_65.pgm      0\n",
      "24      0/137021_28_132_36_36.pgm      0\n",
      "25       0/137021_28_72_70_70.pgm      0\n",
      "26      0/137021_38_124_36_36.pgm      0\n",
      "27       0/137021_39_88_80_80.pgm      0\n",
      "28       0/137021_41_11_63_63.pgm      0\n",
      "29        0/137021_45_1_53_53.pgm      0\n",
      "...                           ...    ...\n",
      "26920    0/txt9_203_101_74_74.pgm      0\n",
      "26921    0/txt9_211_202_36_36.pgm      0\n",
      "26922   0/txt9_221_83_113_113.pgm      0\n",
      "26923    0/txt9_226_196_36_36.pgm      0\n",
      "26924  0/txt9_227_159_117_117.pgm      0\n",
      "26925     0/txt9_227_88_46_46.pgm      0\n",
      "26926     0/txt9_230_11_62_62.pgm      0\n",
      "26927     0/txt9_244_35_74_74.pgm      0\n",
      "26928    0/txt9_246_186_86_86.pgm      0\n",
      "26929     0/txt9_246_23_64_64.pgm      0\n",
      "26930    0/txt9_277_177_36_36.pgm      0\n",
      "26931    0/txt9_30_81_149_149.pgm      0\n",
      "26932      0/txt9_32_76_73_73.pgm      0\n",
      "26933    0/txt9_33_78_157_157.pgm      0\n",
      "26934      0/txt9_36_96_89_89.pgm      0\n",
      "26935      0/txt9_37_31_92_92.pgm      0\n",
      "26936    0/txt9_42_19_165_165.pgm      0\n",
      "26937   0/txt9_44_107_101_101.pgm      0\n",
      "26938      0/txt9_45_41_93_93.pgm      0\n",
      "26939   0/txt9_46_169_106_106.pgm      0\n",
      "26940     0/txt9_53_147_36_36.pgm      0\n",
      "26941     0/txt9_53_168_85_85.pgm      0\n",
      "26942    0/txt9_73_24_170_170.pgm      0\n",
      "26943    0/txt9_79_91_125_125.pgm      0\n",
      "26944   0/txt9_82_128_132_132.pgm      0\n",
      "26945      0/txt9_86_92_67_67.pgm      0\n",
      "26946      0/txt9_87_78_60_60.pgm      0\n",
      "26947     0/txt9_88_209_36_36.pgm      0\n",
      "26948   0/txt9_90_115_110_110.pgm      0\n",
      "26949      0/txt9_9_176_99_99.pgm      0\n",
      "\n",
      "[26950 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "not_face_dataset = ImagesDataset(txt_file='./start_deep/start_deep/negatives.txt',\n",
    "                                    root_dir='./start_deep/start_deep/train_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1 into shape (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-ba679471bc79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnot_face_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnot_face_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-dac32d80aa32>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'labels'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1 into shape (2)"
     ]
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(not_face_dataset)):\n",
    "    sample = not_face_dataset[i]\n",
    "\n",
    "    print(i, sample['name'].shape, sample['label'].shape)\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    show_landmarks(**sample)\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_data = torchvision.datasets.ImageFolder('start_deep/start_deep/train_images/')\n",
    "data_loader = torch.utils.data.DataLoader(imagenet_data, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 91720\n",
      "    Root Location: start_deep/start_deep/train_images/\n",
      "    Transforms (if any): None\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "print(imagenet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x0000014C0EF9F278>\n"
     ]
    }
   ],
   "source": [
    "print(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 : Define neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batc\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 : Define a Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 : Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
